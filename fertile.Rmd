---
title: "Creating optimal conditions for reproducible data analysis in R with `fertile`"

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Audrey Bertin
  thanks: The authors gratefully acknowledge contributions from Hadley Wickham, Jenny Bryan, Greg Wilson, Edgar Ruiz, and other members of the `tidyverse` team. 
  affiliation: Program in Statistical and Data Sciences, Smith College
  
- name: Benjamin S. Baumer
  affiliation: Program in Statistical and Data Sciences, Smith College

keywords:
- reproducibility
- statistical software
- workflow
- collaboration

abstract: |
  
  The advancement of scientific knowledge increasingly depends on ensuring that data-driven research is reproducible: that two people with the same data obtain the same results. However, while the necessity of reproducibility is clear, there are significant behavioral and technical challenges that impede its widespread implementation, and no clear consensus on standards of what constitutes reproducibility in published research. We focus on a series of common mistakes programmers make while conducting data science projects in R, primarily through the RStudio integrated development environment. `fertile` is an R package that operates in two modes: proactively (to prevent reproducibility mistakes from happening in the first place), and retroactively (analyzing code that is already written for potential problems). Furthermore, `fertile` is designed to educate the user about why the mistakes are problematic, and how to fix them. We discuss experimental results from testing `fertile` in an introductory data science course.


bibliography: [bibliography.bib, pkgs.bib]
output: rticles::asa_article
---

```{r, include=FALSE}
knitr::write_bib(c("dplyr", "drake"), file = "pkgs.bib")
```


# Introduction

Data-based research cannot be fully *reproducible* unless the requisite code and data files produce identical results when run by another analyst. 

As research is becoming increasingly data-driven, and because knowledge can be shared worldwide so rapidly, reproducibility is critical to the advancement of scientific knowledge.

When researchers provide the code and data used for their work in a well-organized and reproducible format, readers are more easily able to determine the veracity of any findings by following the steps from raw data to conclusions. 

The creators of reproducible research can more easily receive more specific feedback (including bug fixes) on their work. Moreover, others interested in the research topic can use the code to apply the methods and ideas used in one project to their own work with minimal effort.

However, while the necessity of reproducibility is clear, there are significant behavioral and technical challenges that impede its widespread implementation, and no clear consensus on standards of what constitutes reproducibility in published research (@peng2009bio). Not only are the *components* of reproducible research up for discussion (e.g., need the software be open source?), but the corresponding *recommendations* for ensuring reproducibility also vary (e.g., should raw and processed data files be in separate directories?).

Much of the discussion around reproducibility is also generalized---it is written to be applicable to users working with a variety of statistical software programs. Since all statistical software programs operate differently, generalized recommendations on reproducibility are often shallow and unspecific. While they provide useful guidelines, they can often be difficult to implement, particularly to new analysts who are unsure how to apply such recommendations within the software programs they are using.

Thus, reproducibility recommendations tailored to specific software programs are more likely to be adopted. 

In this paper, we focus on reproducibility in the R programming language with a concentration on projects that use the RStudio integrated development environment.

R is an ideal candidate for reproducibility recommendations due to the language's popularity for statistical analyses and the ease with which analysts can download and begin using the software.  
Several researchers and R users have recognized this and worked to publish a small body of papers and R packages focusing on increasing reproducibility in the R community.

Much of this work is narrowly tailored, with each package effectively addressising a small component of reproducibility--file structure, modularization of code, version control, etc. 
Many existing reproducibility packages, due to their focused nature, succeed at their area of focus well, but at a cost. They are often semi-complex to learn and operate, providing a barrier to entry for less-experienced data analysts. 

There appear to be no existing tools that cater to analysts looking for an easy-to-learn, easy-to-implement, fast way to obtain a broad overview of their projects' reproducibility.
To address this, we present an R package called [`fertile`](https://github.com/baumer-lab/fertile) [^1]:{The authors gratefully acknowledge contributions from Hadley Wickham, Jenny Bryan, Greg Wilson, Edgar Ruiz, and other members of the `tidyverse` team.}, a low barrier-to-entry package which focuses on a series of common mistakes programmers make while conducting data science research in R.


# Literature Review

As is becoming increasingly clear, reproducibility is critically important to the advancement of knowledge in all fields of scientific research. Researchers appear to be acknowledging this, as publications focusing on reproducibility seem to have increased in frequency over the last several years. 

Much of the available literature is focused on the methods for achieving reproducibility in specific disciplines, though much of this information is generalizable to all areas of scientific research. There is also a small body of work focusing on reproducibility within the R community specifically. Some of this work is in the form of academic papers, while other literature comes from the blog posts and websites of several highly-regarded R-focused websites and researchers.  In this section, we will discuss the reproducibility recommendations from both the formal and informal publications, which together influenced which features were included in `fertile`.

There is also a small existing body of R packages focused on providing tools to assist researchers with achieving reproducibility. We will consider the functionality of these packages, identifying `fertile`'s unique role and functionality. 


## Published Literature

As @goodman2016does argue, the language and conceptual framework of research reproducibility vary across the sciences. There are no clear standards agreed upon across fields. 

The variety of recommendations and techniques for achieving reproducibility can be clearly seen Kitzes, Turek, and Deniz' book *The Practice of Reproducible Research*, a collection of case studies on reproducibility practices from across the data-intensive sciences. Although the book idenfies no consensus on exact standards of reproducibility, several common trends and principles emerge from the case studies:

1. Use clear separation, labeling and documentation.
2. Automate processes when possible.
3. Design the data analysis workflow as a sequence of small steps glued together, with outputs from one step serving as inputs into the next.

*A guide to reproducible code in ecology and evolution*, focused on data analysis in R, identifies similar important reproducibility components, reiterating the need to clearly labled, well-documented, and well-separated files. In addition, the paper recommends that a list of dependencies be published and version control be used.

Karl Broman, Professor of Biostatistics and Medical Informatics at the University of Wisconsin, Madison, reiterates the need for clear naming and file separation while sharing several additional suggestions: keep the project contained in one directory, use relative paths, and include a README. 

R OpenSci's reproducibility recommendations share similar components to those discussed previously. They focus on a need for a well-developed file system, with no extraneous files and clear labeling. They also reiterate the need to note dependencies and use automation when possible, while making clear a suggestion not present in the previously-discussed literature: the need to use seeds when running code involving randomness.

When considered in combination, these sources provide a well-rounded picture of the components important to research reproducibility. Using this literature as a guideline, we identify several key features of reproducible work to focus on:

• A well-designed file structure:
  - Separate folders for different file types.
  - No extraneous files.
  - Minimal clutter.
  
• Good documentation:
  - Files are clearly named, preferably in a way where the order in which they should be run is clear.
  - A README is present.
  - Dependencies are noted.
  
• Reproducible file paths:
  - No absolute paths, or paths leading to locations outside of a project's directory, are used in code. Only portable (relative) paths.
  
• Randomness is accounted for:
  - If randomness is used in code, a seed must also be set.
  
• Code conforms to tidyverse style

Much of the available literature focuses on file structure, organization, and naming, and `fertile`'s features are consistent with this. The *ideal* file structure is not agreed upon by academics, though there are some recommendations focused on R that are available. @marwick2018packaging provides the framework for file structure that `fertile` is based on: a structure similar to that of an R package, with an `R` folder, as well as `data`, `data-raw`, `inst` and `vignettes`. 


## R Packages

See also the **drake** package [@R-drake].

rrtools
• Complementary to Ben Marwick's paper
• Creates a basic R package structure for your research and institutes some testthat check functionality


orderly
• Based on the step-by-step analysis process. 
• Build components that take inputs from other components
• Have to declare project structure at the beginning.

workflowr
• Focused a lot on version control and making code available

checkers
• Allows you to create custom checks to look over your work

*Notes: Almost all of these are things you need to do before you start your project. They each focus on a small subset of reproducibility components. They are somewhat complicated to use -- commands are complex and there are often a lot of them.


# Methods

As we have seen, many existing R packages focusing on reproducibility are designed in a way such that they can only be used prior to the start of a project, often for the purposes of creating an organized file structure or ordered file hierarchy. 
`fertile`, on the other hand, can be used at any stage in the data analysis workflow. It is designed to be used on data analyses organized as R Projects (i.e. folders containing a `.Rproj` file). Once an R Project is created, `fertile` can provide benefits at any time, both while coding as well as after the fact.
`fertile` operates in two modes: proactively (to prevent reproducibility mistakes from happening in the first place), and retroactively (analyzing code that has already been written for potential problems). 
`fertile` is available for download at [https://github.com/baumer-lab/fertile](https://github.com/baumer-lab/fertile)

## Proactive Use

Proactively, the package identifies potential mistakes as they are made by the user and outputs an informative message as well as a recommended solution. For example, `fertile` catches when a user passes a potentially problematic file path---such as an absolute path, or a path that points to a location outside of the project directory---to a variety of common input/output functions. 

```{r setup, cache = F, include = FALSE}
knitr::opts_chunk$set(error = TRUE, collapse = TRUE)
```

```{r, include = FALSE, message = FALSE}
readr::write_csv(mtcars, "~/Desktop/my_data.csv")
library(fertile)
library(tidyverse)
```

```{r, message = FALSE}
file.exists("~/Desktop/my_data.csv")
read.csv("~/Desktop/my_data.csv")
```

```{r}
read.table("~/Desktop/my_data.csv")
```


`fertile` is even more aggressive with functions (like `setwd()`) that are almost certain to break reproducibility---it causes them to throw errors.

```{r, message = FALSE}
setwd("~/Desktop")
```


The proactive features are activated immediately after loading the `fertile` package and require no additional effort by the user.

In addition to the interactive warning system, `fertile` provides several useful utility functions. Among other things, these include functions to check the type of a file and a way to create a copy of a project in a temporary directory.

```{r}
is_path_portable("~")

is_data_file("~/Desktop/my_data.csv")
is_image_file("~/Desktop/my_data.csv")
```

```{r}
dir <- getwd()
new_dir <- sandbox(dir)
```

```{r}
dir
fs::dir_ls(dir) %>% head()
```

```{r}
new_dir
fs::dir_ls(new_dir) %>% head()
```

## Retroactive Use

Retroactively, `fertile` analyzes potential obstacles to reproducibility in an RStudio Project (i.e., a directory that contains an `.Rproj` file), including the directory structure, the analyst's use of file paths, randomness, etc. `fertile}` creates reproducibility reports that identify potential mistakes and provide recommendations for remedies. For example, `fertile` might identify the use of randomness in code and recommend setting a seed.

Users can access the majority of `fertile`'s retroactive features through two primary functions.

The `proj_check()` function runs fifteen different reproducibility tests, noting which ones passed, which ones failed, the reason for failure, a recommended solution, and a guide to where to look for help. These tests include: looking for a clear build chain, checking to make sure the root level of the project is clear of clutter, confirming that there are no files present that are not being directly used by or created by the code, and looking for uses of randomness that to not have a call to `set.seed()` present. Subsets of the fifteen tests can be invoked using the select helper functions from `dplyr` and the `proj_check_some()` function.


```{r, eval = FALSE, include = FALSE}
knitr::write_bib("dplyr", file = "pkgs.bib")
```

```{r, message = FALSE}
library(tidyverse)
proj_check_some(".", contains("paths"))
```

The `proj_analyze()` function creates a report documenting the structure of a data analysis project. This report contains informations about all packages referenced in code, the files present in the directory and their types, suggestions for moving files to create a more organized structure, and a list of reproducibility-breaking file paths used in code.


## Sample Use Cases

`fertile` is designed in such a way as to be useful to users of all backgrounds and experiences. In this section, we provide two sample use cases demonstrating situations where users could benefit significantly from using the package. 

### Introductory Data Science Student

Susan is taking an introductory data science course. This is her first time learning how to code, and she has not yet been exposed to ideas of research reproducibility. Her professor has assigned a data analysis project which must be completed in R Markdown. The project requires her to read in a data file located on her computer and use it to produce a graph. 

She reads in the data, makes the graph, and knits her .Rmd file. It compiles successfully, so she submits the assignment. The next day, she receives an email from her professor saying that her assignment failed to compile and that she needs to make changes and try again. 

Susan doesn't understand why it didn't work on the professor's computer when it did on her own. The professor recommends that she install `fertile` and run `proj_check()` on her assignment. She does this and gets a message informing her she used an absolute path to open her dataset, and she should use a relative path. She looks up what this means and then uses the new information to update her assignment. She resubmits and her second attempt is successful. 

On future projects, she always loads and runs fertile to make sure her work is okay before submitting.

### 2. Experienced R User

Emma is a post-doc, with several years of R experience. She is familiar with some basic rules of reproducibility--file paths should always be relative and randomness should always be associated with a seed--but has never needed to pass any sort of reproducibility check before because her professors never emphasized that.

She has just finished a research project and is looking to submit her work to a journal. When researching the journal she is interested in submitting to, she discovers that it has high standards for research reproducibility and a dedicated editor focusing on that aspect of submissions. She goes online and finds the journal's guidelines for reproducibility. They are more complete than any guidelines she has previously been required to conform to. In addition to notes about file paths and randomness, the journal requires a clean, well-organized folder structure, broken down by file category and cleaned of files that do not serve a purpose.

Unsure of the best way to achieve this structure, Emma goes online to find help. In her search, she comes across `fertile`. She downloads the package, and in only a handful of commands, she identifies and removes excess files in her directory and automatically organizes her files into a structure reminiscent of an R package. She now meets the guidelines for the journal and can submit her research.

# Results

In an effort to understand the package's effectiveness, we also share preliminary results from a randomized, controlled experiment conducted on an undergraduate introductory data science course [^2]:{This study is approved by Smith College IRB, Protocol #19-032}. The purpose of the study is to determine whether `fertile` helps students produce data science research that is more likely to be reproducible. 

# Conclusion


`fertile` is an R package that lowers barriers to reproducible data analysis projects in R. 
The features of `fertile` can be accessed almost effortlessly, making it easy for data analysts of all skill levels and backgrounds to gain a better understanding of how to make their work reproducible.
`fertile` is designed to educate the user about why the mistakes are problematic and how to fix them, promoting a greater understanding of reproducibility concepts in its users.
It is written for a wide audience, simple enough to be used by students in an introductory data science course, but still helpful to experienced analysts.
`fertile` also addresses a human challenge of reproducibility. In the moment, it often feels easiest to take a shortcut---to use an absolute path or change a working directory. However, when considering the long term path of a project, spending the extra time to improve reproducibility is worthwhile. `fertile`'s user-friendly features can help data analysts avoid these harmful shortcuts with minimal effort.














































# Introduction

# The Importance of Ensuring Reproducibility

One reason for this is that reproducible research encourages transparency. Providing one's audience with well organized, reproducible code and results allows readers to understand the steps taken to generate findings from raw data and determine for themselves whether or not they believe the results to be reliable or trustworthy. 

Additionally, reproducibility encourages collaboration and extended research, allows others to easily apply the methods used in one project to their own work with minimal effort. @bray2014five

Fertile is designed to make reproducibility simple, providing fast and easy methods to test an R project for reproducibility. The package is intended to be usable by introductory data science students in their first semester of R.











--> make sure this info is included somewhere:

From "Five Concrete Reasons Your Students Should Be Learning to Analyze Data in the Reproducible Paradigm" (@bray2014five)

* Helps with transparency: make clear data cleaning steps between raw data and final data
* Helps with collaboration: easier to share code when it takes very few steps to run on a different computer



The other big ideas on the importance/use of this package (not from sources):

* Make it easier for professors to grade students' code
* Should be usable by intro level data science students
* Possible use by reviewers of journal articles (and by those writing the articles)



# What Defines Reproducibility in Data Science?




# Creating Comprehensive Reproducibility Reports With "fertile"









Main points of different sources, as well as info about how they might be used for the paper.


Here are some sources we might use for motivation behind the project:





### The Reproducibility Crisis

Big idea: most scientific fields are facing a reproducibility crisis and poor statistical use is considered one of the important reasons behind this.

Not sure how useful this is due to the fact that it does not necessarily focus on the same kind of reproducibility we are looking at, which is code reproducibility rather than experimental reproducibility.


@baker20161


### Why is reproducibility important?

From Popper in `The Logic of Scientific Discovery`: "non-reproducible single occurrences are of no significance to science."


@popper2005logic

### What does reproducibility mean in data science?

"The ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results."

@goodman2016does

From "The Practice of Reproducibile Research": Can all of the figures/calculations related to the result be reproduced in a single button press or at least with a reasonably small effort?

@kitzes2017practice

### What makes a data science project reproducibile?

Another way to think about this is: what features does fertile have that match what different sources think needs to be checked to ensure reproducibility?


Some big ideas from the OpenSci discussion page:

* code should use tidy style
* project should have short vignette files -- most written code should be in an R/ directory
* code and data should be stored in separate folders
* a run environment and dependencies should be specified
* there should be a README file
* there should be a data dictionary
* one master script
* code should only use relative paths
* code should be well commented and all variables should be defined

Big ideas from "Packaging data analytical work reproducibly using R"

* research should be organized like an R package!
* clear separation of data, method, and output
* specify the computational environment that was used for the original analysis (typically in a plain text file)
* there should be a README describing the project and where to get started
* script files with reusable functions should go in an R/ directory
* raw data files should be kept in a data/ directory
* analysis scripts and report files should go in an analysis/ directory
* for simpe projects, scripts should be given ascending names. For more complicated ones, the use of some sort of makefile is recommended
* there should also be a DESCRIPTION file with information about the authors, project license, and software dependencies


@marwick2018packaging


From "The Practice of Reproducible Research"

* Are the data openly accessible? If hosted online, is the web address reliable long-term?
* Are they in a commonly used and well-documented file format? Avoid spreadsheets and instead use plain text data if possible!
* Is the raw data available? Is sufficient metadata provided?

* Are dependencies described properly?
* Is full history of source code available through a public version history


* Is there a README?
* Are functions documented?
* Is there narrative documentation explaining how the different pieces work together?
* Are there usage examples?

Folder setup might look like:

1. Raw Data
    + Data
    + README
2. Clean Data
    + Data
3. Results
    + Results file
4. Src
    + Analysis script
    + Script to clean data


@kitzes2017practice


From R OpenSci's Reproducibility Guide:

http://ropensci.github.io/reproducibility-guide/

* Is it clear where to begin?
* Can you determinine which files were used as input to create output files?
* Is there documentation about every result?
* Are exact versions of external applications noted?
* If using randomness, are seeds noted?
* Have you specified a license or noted licenses if you used other people's content?
* Are files easy to find?
* Is it clear what the most recent file is?
* Are there any folders that could be deleted?
* Is analysis output done hierarchically?
* Are there lots of manual data manipulation steps?


R OpenSci also has a discussion of code review practices where users commented. Not too much that was helpful from here (just a few odd comments):
https://discuss.ropensci.org/t/how-do-you-review-code-that-accompanies-a-research-project-or-paper-help-ropensci-plan-a-community-call/1321/43



From "A Guide to Reproducible Code in Ecology and Evolution" (these ideas are pretty universal, though):

A basic project structure:

* The data folder contains all input data (and metadata) used in the analysis.
* The doc folder contains the manuscript
* The figs directory contains figures generated by the analysis
* The output folder contains any type of intermediate or output files (e.g. simulation outputs, models, processed datasets, etc.). You might separate this and also have a cleaned-data folder.
* The R directory contains R scripts with function definitions.
* The reports folder contains RMarkdown files that document the analysis or report on results

* Consistent, ordered naming of scripts
* Use portable paths
* Write unit tests (only for advanced coding)
* Show the packages you used
* Record dependencies and versions of outside things you use



@cooper2017guide


# Karl Broman's Suggestions


* Encapsulate everything within one directory
* Separate raw data from derived data
* Separate data from code
* Use relative paths
* Choose filenames correctly
* Write README files

http://kbroman.org/steps2rr/pages/organize.html




# Why focus on R?

* R is the most popular language for statistical programming and is specifically designed for statistics
* R is great for reprodubility because the code is readable by users, and RMarkdown is a great way to show/explain processes
* R is easy to install and begin using

--> from https://openresearchsoftware.metajnl.com/articles/10.5334/jors.bu/print/



# Similar R Packages

### rrtools

https://github.com/benmarwick/rrtools

* Creates a basic R package named after your research topic
* Generates a license file
* Connects to GitHub and creates a repository
* Generates a README
* Generates a reproducible directory structure
* Creates a dockerfile
* Creates a minimal travis file
* Sets up testthat


### represtools

https://pirategrunt.com/represtools/

Has basic utility functions to support reproducibile research
• Gather
• Cook
• Analyze
• Present

### orderly

https://github.com/vimc/orderly

Focuses on automating a series of basic steps in the prodcess of writing analyses.
• Tracks inputs
• Stores multiple versions of an analysis (unclear whether this is different stages?)
• Track outputs
• Create analyses that depend on the outputs of previous analyses
• You have to declare what you're going to use and create BEFORE you start the analysis

### workflowr

https://f1000research.com/articles/8-1749

Aims to instill a particular workflow that helps make projects more reproducible.

• Version control (git)
• Literate programming (Rmd)
• Automatic checks & safeguards (seemingly related to whether different files in a project have been published on git, and not actually to do with code practices)
• Sharing code / results via browsable website

### checkers

https://github.com/ropenscilabs/checkers

• A framework for reviewing analysis projects
• Seems to be something run after-the-fact
• Currently a work in progress and not passing checks (as of Jan 2019)
• Can create custom checks
• Checks include:
 - Comments
 - Good coding style (lintr)
 - Use of version control
 - Whether preferred packages are used



# Msc sources to look at:


### Victoria Stodden's "Implementing Reproducible Research" book
https://books.google.com/books?hl=en&lr=&id=JcmSAwAAQBAJ&oi=fnd&pg=PP1&dq=Victoria+stodden+reproducibility&ots=ym_gvSxMNH&sig=B5N6FlkoocVjIprIZevtvrL33qQ#v=onepage&q=coombes&f=false







*Packaging data analytical work reproducibly using {R} (and friends) -- Marwick et al*

• The R package can be a suitable template for organizing files into a research compendium to enhance the reproducibility of research.



*Reproducible research and Biostatistics -- Peng*

• To meet the "reproducible" criterion in the biostatistics journal you must submit: a main script and any complementary side scripts, data and any other necessary files, a target file containing results.


*R OpenSci's suggestions:*

• To be reproducible, it should be easy to find files, be clear what each file is, and be easy to know what order to run files. There should be no extraneous files. Everything should be documented, dependencies noted, and seeds used for randomness. Automation should be used when possible.

*A guide to reproducible code in ecology and evolution -- Cooper et al*

• Reproducible work should include a clear and well-separated file system, well-named files, a list of package dependencies, good documentation. Preferably, RMarkdown and version control should be used.


*Karl Broman's suggestions:*

• Keep the project contained in one directory, separate the components (raw data, derived data, code), use relative paths, write a README, and name things clearly.



*What does research reproducibility mean? -- Goodman et al*

•The language and conceptual framework of research reproducibility vary across the sciences. There are no clear standards agreed upon by all scientific researchers.

*The practice of reproducible research: case studies and lessons from the data-intensive sciences -- Kitzes et al*

• Reproducibility examples from a variety of fields all vary significantly but they share key principles / practices in common: 1. Use clear separation, labeling, and documentation. 2. Automate documentation. 3. Design a workflow as a sequence of small steps glued together with intermediate outputs from one step feeding into the next.



